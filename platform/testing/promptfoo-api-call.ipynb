{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "setup-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "EVAL_SERVER_URL = \"http://localhost:15500\"\n",
    "MAX_RETRY_ATTEMPTS = 5\n",
    "RETRY_DELAY = 2  # seconds\n",
    "\n",
    "def check_eval_server():\n",
    "    \"\"\"\n",
    "    Attempts to connect to the evaluation server and returns True if accessible.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{EVAL_SERVER_URL}/eval\")\n",
    "        if response.ok:\n",
    "            print(\"Evaluation server is accessible.\")\n",
    "            return True\n",
    "        else:\n",
    "            raise Exception(\"Server not ready\")\n",
    "    except Exception as e:\n",
    "        print(\"Connection attempt failed:\", e)\n",
    "        return False\n",
    "\n",
    "def generate_config(prompt, model, dataset):\n",
    "    \"\"\"\n",
    "    Mimics the config generation API call using a POST request.\n",
    "    \"\"\"\n",
    "    if not prompt or not model or not dataset:\n",
    "        print(\"Please fill all fields.\")\n",
    "        return\n",
    "    try:\n",
    "        response = requests.post(\"http://127.0.0.1:7100/generate-config/\", json={\n",
    "            \"prompt\": prompt,\n",
    "            \"model\": model,\n",
    "            \"dataset\": dataset\n",
    "        })\n",
    "        print(\"Response:\", response.json())\n",
    "        print(\"Config file generated successfully!\")\n",
    "    except Exception as e:\n",
    "        print(\"Error generating config:\", e)\n",
    "\n",
    "def run_eval(prompt, model, dataset):\n",
    "    \"\"\"\n",
    "    Mimics the evaluation API call using a POST request.\n",
    "    \"\"\"\n",
    "    if not prompt or not model or not dataset:\n",
    "        print(\"Please generate the config first.\")\n",
    "        return\n",
    "    try:\n",
    "        response = requests.post(\"http://127.0.0.1:7100/run-eval/\")\n",
    "        data = response.json()\n",
    "        print(\"Evaluation response:\", data)\n",
    "        if data.get(\"message\") == \"Evaluation completed successfully, and view is running in the background.\":\n",
    "            print(\"Evaluation complete.\")\n",
    "        else:\n",
    "            print(\"Unexpected evaluation response.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error running evaluation:\", e)\n",
    "\n",
    "def wait_for_eval_server():\n",
    "    \"\"\"\n",
    "    Attempts to connect to the evaluation server with retries.\n",
    "    \"\"\"\n",
    "    for attempt in range(1, MAX_RETRY_ATTEMPTS + 1):\n",
    "        print(f\"Connecting to evaluation server... Attempt {attempt}/{MAX_RETRY_ATTEMPTS}\")\n",
    "        if check_eval_server():\n",
    "            print(\"Connected to evaluation server.\")\n",
    "            return True\n",
    "        time.sleep(RETRY_DELAY)\n",
    "    print(\"Unable to connect to evaluation server.\")\n",
    "    return False\n",
    "\n",
    "def manual_retry_connection():\n",
    "    \"\"\"\n",
    "    Manually retries connecting to the evaluation server.\n",
    "    \"\"\"\n",
    "    print(\"Retrying connection...\")\n",
    "    check_eval_server()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generate-config-md",
   "metadata": {},
   "source": [
    "### Generate Config\n",
    "\n",
    "This cell calls the `generate_config` function to generate a configuration file via the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "generate-config-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: {'message': 'Error pulling model', 'error': 'the input device is not a TTY\\n'}\n",
      "Config file generated successfully!\n"
     ]
    }
   ],
   "source": [
    "# Replace these values with your actual input\n",
    "prompt = \"Your prompt here\"\n",
    "model = \"smollm2\"\n",
    "dataset = \"Your google sheet dataset URL\"\n",
    "\n",
    "generate_config(prompt, model, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-eval-md",
   "metadata": {},
   "source": [
    "### Run Evaluation\n",
    "\n",
    "This cell calls the `run_eval` function to initiate the evaluation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-eval-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_eval(prompt, model, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "check-eval-server-md",
   "metadata": {},
   "source": [
    "### Check Evaluation Server\n",
    "\n",
    "This cell attempts to connect to the evaluation server, retrying automatically if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-eval-server-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "wait_for_eval_server()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
